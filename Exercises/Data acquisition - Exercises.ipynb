{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data acquisition - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9.1.\n",
    "\n",
    "The list below contains a number of URLs. They are the web addressed of texts created for the Project Gutenberg website.   \n",
    "\n",
    "```\n",
    "urls = [ 'http://www.gutenberg.org/files/580/580-0.txt' ,\n",
    "'http://www.gutenberg.org/files/1400/1400-0.txt' ,\n",
    "'http://www.gutenberg.org/files/786/786-0.txt' ,\n",
    "'http://www.gutenberg.org/files/766/766-0.txt' \n",
    "]\n",
    "```\n",
    "\n",
    "Write a program in Python which can download all the files that are listed. As filenames, use the same names that are used by Project Gutenberg (e.g. '580-0.txt' or '1400-0.txt'). The basename in a URL can be extracted using the os.path.basename() function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os \n",
    "\n",
    "urls = [ 'http://www.gutenberg.org/files/580/580-0.txt' ,\n",
    "'http://www.gutenberg.org/files/1400/1400-0.txt' ,\n",
    "'http://www.gutenberg.org/files/786/786-0.txt' ,\n",
    "'http://www.gutenberg.org/files/766/766-0.txt' \n",
    "]\n",
    "\n",
    "\n",
    "for text in urls:\n",
    "    response = requests.get(text)\n",
    "    response.encoding = 'utf-8'\n",
    "    out = open( os.path.basename(text) , 'w' )\n",
    "    out.write( response.text )\n",
    "    out.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9.2.\n",
    "\n",
    "Write Python code which can download the titles and the URLs of Wikipedia articles whose titles contain the word 'Dutch'. Your code needs to display the first 30 results only.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "baseURL = 'https://en.wikipedia.org/w/api.php?action=opensearch'\n",
    "\n",
    "searchTerm = \"Dutch\"\n",
    "limit = 30\n",
    "format = 'json'\n",
    "\n",
    "apiCall = '{}&search={}&limit={}&format={}'.format( baseURL, searchTerm , limit , format )\n",
    "\n",
    "\n",
    "responseData = requests.get( apiCall )\n",
    "\n",
    "wikiResults = responseData.json()\n",
    "\n",
    "\n",
    "for i in range( 0 , len(wikiResults[1]) ):\n",
    "    print( 'Title: ' + wikiResults[1][i] )\n",
    "    print( 'Tagline: ' + wikiResults[2][i] )\n",
    "    print( 'Url: ' + wikiResults[3][i] + '\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9.3.\n",
    "\n",
    "Write an application in Python which can extract all the publications that have been added to a specific ORCID account. Make use of the ORCID API to do this. Information about individual ORCID accounts can be obtained by appending these to the following base URL: https://pub.orcid.org/v2.0/. The ORCID API returns data in XML. The list of publications can be found underneath \"record/activities-summary/works/group\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orcid = '0000-0002-8469-6804'\n",
    "\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "ns = {'o': 'http://www.orcid.org/ns/orcid' ,\n",
    "'s' : 'http://www.orcid.org/ns/search' ,\n",
    "'h': 'http://www.orcid.org/ns/history' ,\n",
    "'p': 'http://www.orcid.org/ns/person' ,\n",
    "'pd': 'http://www.orcid.org/ns/personal-details' ,\n",
    "'a': 'http://www.orcid.org/ns/activities' ,\n",
    "'e': 'http://www.orcid.org/ns/employment' ,\n",
    "'c': 'http://www.orcid.org/ns/common' , \n",
    "'w': 'http://www.orcid.org/ns/work'}\n",
    "\n",
    "\n",
    "try:\n",
    "    orcidUrl = \"https://pub.orcid.org/v2.0/\" + orcid\n",
    "    print( orcidUrl )\n",
    "    \n",
    "    response = requests.get( orcidUrl )\n",
    "    root = ET.fromstring(response.text)\n",
    "    \n",
    "    creationDate = root.find('h:history/h:submission-date' , ns ).text\n",
    "    \n",
    "    print('\\nORCID created on:')\n",
    "    print(creationDate)\n",
    "    \n",
    "    print('\\nWorks:')\n",
    "    \n",
    "    works = xml.findall('a:activities-summary/a:works/a:group' , ns )\n",
    "    for w in works:\n",
    "        title = w.find('w:work-summary/w:title/c:title' , ns ).text\n",
    "        print(title)\n",
    "        doiEl = w.find('c:external-ids/c:external-id/c:external-id-url' , ns )\n",
    "        if doiEl is not None:\n",
    "            doi = doiEl.text\n",
    "            print(doi)\n",
    "            \n",
    "except:\n",
    "    print(\"Data could not be downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 9.4.\n",
    "\n",
    "The API developed by [OpenStreetMap](https://www.openstreetmap.org/) can be used, among other things, to find the precise geographic coordinates of a specific location. The base URL of this API is https://nominatim.openstreetmap.org/search. Following the 'q' parameter, you need to supply a string describing the locations whose latitude and longitude you want to find. As values for the 'format' parameter, you can use 'xml' or 'json'. Use this API to find the longitude and the latitude of the addresses in the following list:\n",
    "\n",
    "```\n",
    "addresses = ['Grote Looiersstraat 17 Maastricht' , 'Witte Singel 27 Leiden' , 'Singel 425 Amsterdam' , 'Drift 27 Utrecht' , 'Broerstraat 4 Groningen']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import string\n",
    "from os.path import isfile, join , isdir\n",
    "import os\n",
    "\n",
    "addresses = ['Grote Looiersstraat 17 Maastricht' , 'Witte Singel 27 Leiden' , 'Singel 425 Amsterdam' , 'Drift 27 Utrecht' , 'Broerstraat 4 Groningen']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for a in addresses:\n",
    "    url = 'https://nominatim.openstreetmap.org/search?q='+ a + '&format=xml'\n",
    "    url = re.sub( '\\s+' , '%20' , url )\n",
    "\n",
    "    response = requests.get( url )\n",
    "    root = ET.fromstring( response.text )\n",
    "    el = root.findall('place')\n",
    "    \n",
    "    count = 0\n",
    "    if el is not None:\n",
    "        for place in el:\n",
    "            count += 1\n",
    "            lat = place.attrib['lat']\n",
    "            lon = place.attrib['lon']\n",
    "            if count == 1:\n",
    "                print( '{}: {},{}\\n'.format( a, lat , lon ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9.5.\n",
    "\n",
    "Extract the titles of all the movies which are included on the [list of top rated movies](https://www.imdb.com/chart/top?ref_=ft_250) using web scraping. Also extract the URL of the webpage on IMDB descriobing these movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "\n",
    "soup = \"\"\n",
    "\n",
    "\n",
    "url = 'https://www.imdb.com/chart/top?ref_=ft_250'\n",
    "\n",
    "\n",
    "response = requests.get( url )\n",
    "soup = BeautifulSoup( response.text ,\"lxml\")\n",
    "\n",
    "\n",
    "movies = soup.find_all('td', {'class': 'titleColumn'})\n",
    "\n",
    "for m in movies:\n",
    "    children = m.findChildren(\"a\" , recursive=False)\n",
    "    for c in children:\n",
    "        movieTitle = c.text\n",
    "        url = c.get('href')\n",
    "        url = 'http://imdb.com' + url\n",
    "        print( '{}: {}'.format( movieTitle , url ) )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
