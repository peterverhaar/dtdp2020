{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Lexicons\n",
    "\n",
    "When humanities studies concentrate exclusively on vocabulary or on the syntax of texts, and not so much on the meaning of these text, this is admittedly a rather shallow form of research. Various attempts have\n",
    "been made, however to bridge the gap between the lexical codes and their semantic contents. One of the ways in which the semantic aspects of texts may be uncovered is by making use of lexicons which map the text’s tokens to pre-defined semantic categories. Examples of applications in which this principle is implemented include\n",
    "the [Harvard General Inquirer (HGI)](http://www.wjh.harvard.edu/~inquirer/homecat.htm), [the Linguistic Inquiry and Word Count (LIWC)\n",
    "tool](http://liwc.wpengine.com/)  and the [UCREL Semantic Analysis System (USAS)](http://ucrel.lancs.ac.uk/usas/). The programmers responsible for the *Harvard General Inquirer*, for example, have defined 182 semantic categories, and they have compiled long list of words that are indicative of these categories. The category “negative”, for instance, contains over 2290 entries. Such word lists are usually referred to as lexicons. \n",
    "\n",
    "To let you work the possibilities of semantic tagging, a number of the lexicons that have been made available have been downloaded and merged. Next to the lexicons developed for the HGI and USAS, the word lists created for this course also include terms taken from lists compiled by [Bing Liu](https://www.cs.uic.edu/~liub/) and by the project team that worked on the [Multi-Perspective Question Answering (MPQA) tool](http://mpqa.cs.pitt.edu/). \n",
    "\n",
    "The merged semangic lexicons have been added to the github repository for DTDP: \n",
    "https://raw.githubusercontent.com/peterverhaar/dtdp2020/master/Lexicons/\n",
    "\n",
    "Among other word lists, this folder contains a lexicon named \"positive.txt\", which lists words with a positive connotation. This particular lexicon can be used to examine the degree to which the text expresses positive emotions. \n",
    "\n",
    "The lexicons can all be downloaded by running the code in the following two cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtdpTdm as tdm\n",
    "import os\n",
    "from os.path import join\n",
    "import requests\n",
    "import re\n",
    "\n",
    "lexicons = [ 'positive.txt', 'negative.txt' , 'Abstract.txt' , 'Academic.txt' , 'Active.txt' , 'Economics.txt' , 'Hostile.txt' , 'Increase.txt' , 'Legal.txt' , 'Military.txt' , 'Movement.txt' , 'Pain.txt' , 'Passive.txt' , 'Pleasure.txt' , 'Politics.txt' , 'Power.txt' , 'Religion.txt' , 'Space.txt' , 'Time.txt' , 'Transportation.txt' , 'Vice.txt' , 'Weather.txt' , 'workAndEmployment.txt' ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseUrl = 'https://raw.githubusercontent.com/peterverhaar/dtdp2020/master/Lexicons/'\n",
    "\n",
    "def downloadLexicon(fileName):\n",
    "    response = requests.get( baseUrl + fileName)\n",
    "    if response:\n",
    "        response.encoding = 'utf-8'\n",
    "        out = open( fileName , 'w' , encoding = 'utf-8' )\n",
    "        out.write( response.text )\n",
    "        out.close()\n",
    "    else:\n",
    "        print('Cannot download lexicon file!')\n",
    "\n",
    "\n",
    "\n",
    "for l in lexicons:\n",
    "    print( 'Downloading {} ...'.format(l) )\n",
    "    downloadLexicon( l )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the lexicons are all available on your computer, you can use the code below to count the number of occurrences of the words in these various lexicons within the texts of your corpus. The code makes use of the method `countOccurrencesLexicon()` from the `dtdpTdm` module. The result (consisting of counts for all the texts in your corpus) is stored in a file named 'lexicon.csv'.\n",
    "\n",
    "If your texts are long, or if the corpus contains many texts, running the code make take quite a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing semantic tagging for ARoomWithaView.txt ...\n",
      "Lemmatising Corpus/ARoomWithaView.txt ...\n",
      "Performing semantic tagging for ATaleofTwoCities.txt ...\n",
      "Lemmatising Corpus/ATaleofTwoCities.txt ...\n",
      "Performing semantic tagging for HeartofDarkness.txt ...\n",
      "Lemmatising Corpus/HeartofDarkness.txt ...\n",
      "Performing semantic tagging for Ivanhoe.txt ...\n",
      "Lemmatising Corpus/Ivanhoe.txt ...\n",
      "Performing semantic tagging for MobyDick.txt ...\n",
      "Lemmatising Corpus/MobyDick.txt ...\n"
     ]
    }
   ],
   "source": [
    "csv = open( 'lexicon.csv' , 'w' , encoding = 'utf-8' )\n",
    "\n",
    "## print header\n",
    "csv.write( 'title' )\n",
    "for l in lexicons:\n",
    "    column = re.sub(r'\\.txt' , '' , l)\n",
    "    csv.write(',{}'.format(column.lower() ) )\n",
    "csv.write('\\n')\n",
    "\n",
    "dir = 'Corpus'\n",
    "for file in os.listdir( dir ):\n",
    "    if re.search( r'\\.txt$' , file ):\n",
    "        print( 'Performing semantic tagging for {} ...'.format( file ) )\n",
    "        csv.write( tdm.getTitle( file ) )\n",
    "        path = join( dir, file )\n",
    "        tokens = tdm.numberOfTokens(path)\n",
    "        for l in lexicons:\n",
    "            count = tdm.countOccurrencesLexicon( path , l )\n",
    "            csv.write( ',{}'.format( count / tokens ) )\n",
    "        csv.write('\\n')\n",
    "        \n",
    "csv.close()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, the counts that have made for the terms on the various lexicons can be visualised as a bar chart. A value of the variable named `y`, you need to type in the name of the lexicon, without the .txt extension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('lexicon.csv')\n",
    "\n",
    "fig = plt.figure( figsize=( 7 ,6 ) )\n",
    "ax = plt.axes()\n",
    "\n",
    "x = 'title'\n",
    "y = 'religion'\n",
    "\n",
    "\n",
    "bar_width = 0.45\n",
    "opacity = 0.8\n",
    "\n",
    "ax.bar( df[x] , df[y] , width = bar_width, alpha = opacity , color = '#27a897')\n",
    "\n",
    "plt.xticks(rotation= 90)\n",
    "\n",
    "ax.set_xlabel('Categories' , fontsize= 12)\n",
    "ax.set_ylabel('Mean values' , fontsize = 12 )\n",
    "ax.set_title( y.title() , fontsize=20 )\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was explained in the notebook on [Natural Language Processing](NLTK.ipynb), it is also possible to explore the broader differences between the various categories that can be distinguished in your corpus. To connect the lexicon CSV to your metadata CSV which captures data about these categories, you need to run the following code.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv( 'lexicon.csv' )\n",
    "metadata = pd.read_csv( 'metadata.csv' )\n",
    "df['class'] = metadata['class']\n",
    "\n",
    "df.to_csv (r'dataset.csv', index = False, header=True) \n",
    "\n",
    "df = pd.read_csv( 'dataset.csv' )\n",
    "\n",
    "colours = [ '#09349E' , '#D1AC32' , '#C70C2B' , '#6AD964' , '#A640E6' ]\n",
    "\n",
    "classColours = dict()\n",
    "\n",
    "unique_categories = list( set( df['class'] ) )\n",
    "if len( unique_categories ) <= len(colours):\n",
    "    for u in range( len( unique_categories ) ):\n",
    "        classColours[ unique_categories[u] ] = colours[u]\n",
    "else:\n",
    "    print(\"You have more than five categories. You need to add colours to the list!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates a bar chart whose colours represent the various categories you have distinguished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "colours = []\n",
    "for category in df['class']:\n",
    "    colours.append( classColours[category] )\n",
    "\n",
    "fig = plt.figure( figsize=( 7 ,6 ) )\n",
    "ax = plt.axes()\n",
    "\n",
    "x = 'title'\n",
    "y = 'religion'\n",
    "\n",
    "\n",
    "bar_width = 0.45\n",
    "opacity = 0.8\n",
    "\n",
    "ax.bar( df[x] , df[y] , width = bar_width, alpha = opacity , color = colours )\n",
    "\n",
    "plt.xticks(rotation= 90)\n",
    "\n",
    "\n",
    "patchList = []\n",
    "for key in classColours:\n",
    "    data_key = mpatches.Patch(color=classColours[key], label=key)\n",
    "    patchList.append(data_key)\n",
    "    \n",
    "plt.legend(handles=patchList , shadow=True, fontsize='large' , frameon = True )\n",
    "\n",
    "\n",
    "ax.set_xlabel('Categories' , fontsize= 12)\n",
    "ax.set_ylabel('Mean values' , fontsize = 12 )\n",
    "ax.set_title( y.title() , fontsize=20 )\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below, finally, creates a bar chart which visualises the counts for three lexicons simultaneously. The counts to be shown need to be specified in the list named `lexiconsInChart`. This bar chart enables you to cmpare the values collected for the three semantic domains you have listed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('lexicon.csv')\n",
    "\n",
    "fig = plt.figure( figsize=( 12 ,8 ) )\n",
    "ax = plt.axes()\n",
    "\n",
    "lexiconsInChart = [ 'academic' , 'time' , 'religion' ]\n",
    "\n",
    "\n",
    "ind = np.arange( df.shape[0] )\n",
    "\n",
    "\n",
    "width = 0.27     \n",
    "\n",
    "\n",
    "bar1 = ax.bar(ind, df[ lexiconsInChart[0] ] , width, color='#3264a8')\n",
    "bar2 = ax.bar(ind+width, df[lexiconsInChart[1] ], width, color='#edcf0e')\n",
    "bar3 = ax.bar(ind+width*2, df[ lexiconsInChart[2] ], width, color='#e01c0d')\n",
    "\n",
    "\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_xticks(ind+width)\n",
    "\n",
    "ax.set_xticklabels( df['title'] )\n",
    "ax.legend( ( bar1[0], bar2[0], bar3[0]), ( lexiconsInChart[0] , lexiconsInChart[1] , lexiconsInChart[2] ) )\n",
    "\n",
    "\n",
    "plt.xticks(rotation= 90)\n",
    "\n",
    "ax.set_xlabel('Categories' , fontsize= 12)\n",
    "ax.set_ylabel('Mean values' , fontsize = 12 )\n",
    "ax.set_title( 'Sample data' , fontsize=20 )\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
