{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation\n",
    "\n",
    "Many of the functionalities that can be offered by text analysis tools are based on\n",
    "counts of the smaller linguistic units that occur within texts, such as their words or their sentences. \n",
    "This preparatory\n",
    "process, in which the program divides the linear texts into discrete units, is generally referred to as “segmentation” or “tokenisation”. Segmentation generally takes place on the basis of the spaces, the punctuation marks and the line breaks that can be found within texts. Utilising\n",
    "such notational conventions, which can be referred to as ‘soft markup’, text mining\n",
    "applications can be developed for the recognition of units such as words, sentences\n",
    "or paragraphs. \n",
    "\n",
    "The individual words that are found are commomly referred to as “**tokens**\". When this full list is deduplicated, leaving only the unique words, the items in such lists are called “**types**”. Frequency lists, counting occurrences of types, often form the basis for further statistical analyses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains the function named `tokenise()` from the dtdpTdm module. It demand a certain string as input. The first line creates an empty list which will eventually receive all the tokens in this list. In the first few lines, the sting that is supplied is converted into lower case, and all occurrences of hyphens or dashes are surrounded by spaces. In some texts, these punctuation marks are used to separate words. Next, the string is divided into its separate words using the `split()` method from the `re` module. From all the words that are found in this way, leading an trailing punctuation is removed using the standard `strip` method, which is available for all strings. The `strip()` method in the code below removes all the symbols that are predefined in the Python language as `string.punctuation`. It was decided, finally, that words should minimally contain one alphabetic character. The condition given after the `if` keyword stipulates that substrings can only be appended to the `tokens` list when they contain at least one character from our alphabet, either in upper or lower case.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "# function to tokenise a string into words\n",
    "def tokenise( text ):\n",
    "    tokens = []\n",
    "    text = text.lower()\n",
    "    text = re.sub( '--' , ' -- ' , text)\n",
    "    text = re.sub( '-' , ' - ' , text)\n",
    "    words = re.split( r'\\s+' , text )\n",
    "    for w in words:\n",
    "        w = w.strip( string.punctuation )\n",
    "        if re.search( r\"[a-zA-Z']\" , w ):\n",
    "            tokens.append(w)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test this function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtdpTdm as tdm\n",
    "import string\n",
    "\n",
    "sentence = 'How many words are there in this sentence?'\n",
    "\n",
    "words = tdm.tokenise(sentence)\n",
    "\n",
    "    \n",
    "print('The sentence contains {} tokens:'.format( len(words) ) )\n",
    "\n",
    "for w in words:\n",
    "    print(w)\n",
    "\n",
    "print( '\\nThe variable string.punctuation contains the following characters: \\n{}'.format( string.punctuation )   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tokenise()` function, defined in the `dtdpTdm` module, can obviously be very helpful when we want to calculate word frequencies. To keep track of the word counts, it can be very useful to work with a Python dictionary. As explained in the tutorial, a dictionary is variable which can hold value pairs. In this situation, we also want to store pairs of values: (1) the types found in the text and (2) the number of tims thes types occur. The code below defines the words (i.e. the types) as the indices of the dictionary named `freq`, and the values that these indices are associated represent the number of times these words are found in the text. \n",
    "\n",
    "The following code reads in a spefific text line by line. It firstly tokenises each line, using the `tokenise()` method. Each word that is found in this way will become an index in the `freq` dictionary. At the first occurrence of this word, the word will be assigned the value 1. The value will be updated, however, with every new occurrence of the owrd in the text. The program does this for each individual word. Once Python has read the full text, it will have data about the occurrences of all the words in this text. \n",
    "\n",
    "The final few lines print the 30 most frequent words. The number of words to be shown can be manipulated by changing the value of the variable named `max`.  \n",
    "\n",
    "To calculate a frequency list for one of the texts in your own corpus, you obviously need to change the value of the file variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import re\n",
    "import dtdpTdm as tdm\n",
    "\n",
    "# create variable for the name of the folder containing your texts\n",
    "dir = 'Corpus'\n",
    "\n",
    "# create variable for the name of file you want to analyse\n",
    "file = 'ATaleofTwoCities.txt'\n",
    "\n",
    "# create a dictionary which will count the tokens\n",
    "# word as index, and frequency as value\n",
    "freq = dict()\n",
    "\n",
    "# read the text\n",
    "text = open( join( dir, file ) )\n",
    "\n",
    "\n",
    "# Read the text, and iterate through the text line by line. \n",
    "for line in text:\n",
    "    # Tokenise each line, and update the dictionary as you go along\n",
    "    words = tdm.tokenise( line )  \n",
    "    for w in words:\n",
    "        freq[w] = freq.get( w, 0 ) + 1\n",
    "         \n",
    "count = 0 \n",
    "max = 30        \n",
    "for word in reversed( tdm.sortedByValue(freq) ):\n",
    "    count += 1\n",
    "    print( '{}. {} => {}'.format( count , word , freq[word] ) )\n",
    "    if count == max:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have managed to run the code above, you shall probay see that the list that is created is headed by so-called functional words. These are words such as articles, prepositions and pronouns which are important for producting grammatically correct sentences, but which mostly have little expessive value by themselves. If you are interested in studying the contents of a text, it can be useful to remove such frequently used functional words. The `nltk` library can help you with this. It contain a module named `stopwords`, which includes a predefined list of frequently used function words. \n",
    "\n",
    "The code below gives a demonstration of this. It firstly creates a list named `stopwords`, on the basis of the predefined list from the `nltk` library. Next, it created a copy of the `freq` dictionary that was created earlier, and it saves the copy under the name `freq_copy`. Next, while iterating through the `freq_copy` dictionary, all the words that are on the list of stopwords are removed from the original `freq` dictionary. \n",
    "\n",
    "When you run the coe below, you should see an updated list, without the frequently used function words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "# print(stopwords)\n",
    "\n",
    "\n",
    "freq_copy = freq.copy()\n",
    "\n",
    "for w in freq_copy:\n",
    "    if w in stopwords:\n",
    "        del freq[w]\n",
    "        \n",
    "        \n",
    "count = 0 \n",
    "max = 30        \n",
    "# print the 30 most common words       \n",
    "for word in reversed( tdm.sortedByValue(freq) ):\n",
    "    count += 1\n",
    "    print( '{}. {} => {}'.format( count , word , freq[word] ) )\n",
    "    if count == max:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regular word lists, frequencies are usually distributed according to Zipf’s Law, which states that there are normally small numbers of words that occur very frequently, and large numbers of words with low frequences or so-called hapax legomena, which are words that occur only once. As was explained, the high-frequency words are usually function words. \n",
    "\n",
    "These highly frequent words can indeed be removed by working with a list of stopwords. As an alternative, we can work with the term frequency-inverse document frequency formula (tf-idf). It is a statistical operation which\n",
    "was designed to indicate the significance or the relative uniqueness of a specific term\n",
    "within the context of a corpus. \n",
    "\n",
    "<img src=\"https://www.seoadviesmkb.nl/images/blog/tf-idf.png\" style=\"width: 250px; \"/>\n",
    "\n",
    "The tf-idf formula assigns weights\n",
    "to the bare counts of the words. These weights are calculated by firstly dividing the total number of texts in the corpus by the total number of texts that contain a given word. If the word occurs in all the texts of the corpus, this number will be one. If the term occurs in only one text, this number will obviously be higher. To amplify the effects of differences such as these, the formula works with the logarithm of the division that was discussed. As log(1) equals to zero, words which occur in all texts are effctively removed. The formula can thus be use to retrieve the rarer, more distinctive words from a certain text. \n",
    "\n",
    "The dtdpTdm module has a method which can be used calculate frequencies following this tdf-Idf formula. As parameters, it demands (1) a reference to the directory containing the full corpus, and (2) the name of the file containing the text you want to analyse. The method returns a dictionary containing all the weighted word frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtdpTdm as tdm\n",
    "\n",
    "dir = 'Corpus'\n",
    "text = 'ATaleofTwoCities.txt'\n",
    "\n",
    "freq = tdm.tdIdf( dir , text )\n",
    "\n",
    "for w in reversed( tdm.sortedByValue(freq) ):\n",
    "    print( w + ' => ' + str(freq[w]) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concordances\n",
    "\n",
    "To examine *how* words are used in a text, it can be useful to create a concordance. Such resources are sometimes referred to as keyword in context lists. In a concordance, all the occurrences of a given search term are shown in combination with words that occur before and after this term. \n",
    "\n",
    "The dtdpTdm module has a method name `concordance()` which can help you to create such a concordance. To work with it, you need to supply three parameters: (1) the text that you want to analyse, i.e. its filename; (2) a regular expression representing the word(s) whose contexts you want to examine; and (3) a number that specifies the extent of the context. The number that is given as the third parameter determines the number of words that will be shown before and after the search result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import dtdpTdm as tdm\n",
    "\n",
    "dir = 'Corpus'\n",
    "fileName = 'MobyDick.txt'\n",
    "\n",
    "fullPath = join( dir , fileName )\n",
    "\n",
    "## the next line produces a list of all lines containing the regular expression\n",
    "## formatted as a KWIC list\n",
    "c = tdm.concordance( fullPath , r'\\bwhales?' , 25 )\n",
    "\n",
    "for line in c:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocation analysis\n",
    "\n",
    "Collocation analyses focus on the words that are used in the vicinity of a provided search term. Such analyses give an impression of the semantic contexts of these term. In the dtdpTdm module, collocations can be analysed using the `collocation()` method. The parameters are the same as those of the `concordance()` method. \n",
    "\n",
    "The function `removeStopWords()` can also be useful in this context. It removes the most common words from given dictionary. Analogous to the situation that was discussed earlier, it makes use of the list of stopwords defined in the `nltk` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import dtdpTdm as dtdp\n",
    "\n",
    "dir = 'Corpus'\n",
    "fileName = 'MobyDick.txt'\n",
    "fullPath = join( dir , fileName )\n",
    "\n",
    "\n",
    "freq = dtdp.collocation( fullPath , r'whales?' , 30 )\n",
    "freq = dtdp.removeStopWords(freq)\n",
    "\n",
    "for w in freq:\n",
    "    print(w , freq[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below can be used to visualise a given dictionary with word frequencies into a word cloud. It can be used, in fact, to visualise the results of the basic frequency lists, the lists created using the td-idf formula, and the collocation analysis. \n",
    "\n",
    "To make sure that the word cloud works with the correct dictionary, firstly run the code in of the cells above, and run the code for making the word cloud immediately after this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from wordcloud import WordCloud \n",
    "\n",
    "wordcloud = WordCloud( background_color=\"white\",  width=1500,height=1000, max_words= 100,relative_scaling=1,normalize_plurals=False).generate_from_frequencies(freq)\n",
    "\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the code above produced an error message, this may be caused by the fact that wordcloud module has not been inestalled yet on your computer. To get the code above to work, you may need to install the wordcloud module using the commands in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using code that was explained in Part 4 of the Python tutorial, can you visualise the 50 most frequent words as a bar chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dispersion graphs\n",
    "\n",
    "Frequencies can also be clarified in dispersion graphs. Instead of giving information about the total number of occurrences, dispersion graphs indicate the number of occurrences in separate sections of the texts. While it is possible to calculate frequencies within, for example, the separate chapters of a novel, such dispersion graphs often work with random segments, simply created by chopping up the text into chunks of a given number of words. \n",
    "\n",
    "To produce such graphs, a full text firstly needs to be divided into segments. The graph that is generated provides information frequency of a particular word within each of these segments. The graph can be used to locate those sections in which a given term is used most frequently.\n",
    "\n",
    "The method `divideIntoSegments()` demands two parameters. The first of these is the text to be analysed. Via the second parameter, you can specify the number of segments that need to be created. The number of segments als determines the size of the segments. The number of these segments are calculated by dividing the total number of tokens by the number of segments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from os.path import join\n",
    "import dtdpTdm as tdm\n",
    "\n",
    "dir = 'Corpus'\n",
    "fileName = 'MobyDick.txt'\n",
    "fullPath = join( dir , fileName )\n",
    "\n",
    "segments = tdm.divideIntoSegments( fullPath , 30 )\n",
    "\n",
    "data = dict()\n",
    "\n",
    "count = 0\n",
    "for s in segments:\n",
    "    count += 1\n",
    "    hits = re.findall( r'\\bwhale' , s , re.IGNORECASE )\n",
    "    data[ count ] = len( hits )\n",
    "    \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "fig = plt.figure( figsize=( 12 , 4 ) )\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.plot( list(data.keys() ) , list( data.values() ) , color = '#930d08' , linestyle = 'solid')\n",
    "\n",
    "ax.set_xlabel('Section')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "ax.set_title( 'Dispersion graph for \"Moby Dick\" ')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this notebook can help you to examine the vocabulary in a single text. The notebook named '[NTLK.ipynb](NLTK.ipynb)' enables you to create quantitative data about all the texts in your corpus, and to do compare your texts on the basis of these data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
