{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate analysis\n",
    "\n",
    "The previous sections of this tutorial have explained how you can use Python to produce quantitative data about a wide range of textual phenomena. Using libraries such as `nltk` and other modules, you can produce counts of the number of sentences, syllables, words, nouns and adjectives, among many other aspects. Analyses such as these can result in large data sets, comprising many different variables. \n",
    "\n",
    "Within such large data sets, it can be difficult to explore these many rows and columns effectively. Which variables are correlated? Which variables are most important if we want study the differences between texts in different genres? Visual displays such as bar charts of scatter plots can be useful to examine such differences, but we can normally use these to explore the values for one two variables only.\n",
    "\n",
    "To be able to examine the effects of multiple variables simultaneously, we really need to work with multivariate analyses such as *Principal Component Analysis* or *Hierarchical Cluster Analysis*. It can also be useful to examine the correlations between the variables in your data set.\n",
    "\n",
    "## Correlations\n",
    "\n",
    "The code below firstly combines the data in the CSV files named 'metadata.csv', 'nltk.csv' and 'lexicon.csv' into one larger data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "combined = pd.read_csv( 'nltk.csv' )\n",
    "\n",
    "df2 = pd.read_csv( 'lexicon.csv' )\n",
    "\n",
    "columns = df2.columns.tolist() \n",
    "columns.remove('title')\n",
    "\n",
    "for c in columns:\n",
    "    combined[c] = df2[c]\n",
    "    \n",
    "    \n",
    "df = pd.read_csv( 'metadata.csv' )\n",
    "\n",
    "unique_categories = list( set( df['class'] ) )\n",
    "\n",
    "for u in unique_categories:\n",
    "    values = []\n",
    "    for index, row in df.iterrows():\n",
    "        if row['class'] == u:\n",
    "            values.append(1)\n",
    "        else:\n",
    "            values.append(0)\n",
    "    combined[u] = values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can produce a heatmap displaying all the correlations between the variables in this data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "ax = sns.heatmap( combined.corr() , linewidth=0.5 , cmap=\"YlGnBu\" )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the diagram that can be created using the code that is supplied suggests that two variables are strongly correlated, it can be useful to explore these varibales in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis\n",
    "\n",
    "When you want to explore multiple variables simultaneously, it can be useful to apply Principal Component Analysis (PCA). PCA is a form of multi-variate analysis in which a large number of variables can be replaced by a much smaller number of variables. The method aims to create new variables which can account for most of the variability in the full data set. These new variables are referred to as the principal components. If the first two principal components account for most of the variability, the global distribution of the values in yout data can be clarified by plotting these two principal components on a scatter plot.\n",
    "\n",
    "The principal components can only be calculated accurately when all the variables that are analysed are of the same data type and when they all have the same units of measurement. When your data set contains a mixture of data types (e.g. strings for the titles of novels and floating point numbers for the average number of words per sentence) and varying units of measurement, it can be useful to derive a new data frame from the original data frame, and to make sure that the units of measurement of all the variables in the new data frame are consistent. The code below firstly filters the existing `combined` data frame. It only retains those columns which are in the list that is mentioned within the `filter()` method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.filter ( [ 'adjectives',\n",
    " 'adverbs',\n",
    " 'nrSyllables',\n",
    " 'positive',\n",
    " 'negative',\n",
    " 'abstract',\n",
    " 'academic',\n",
    " 'active',\n",
    " 'economics',\n",
    " 'hostile',\n",
    " 'increase',\n",
    " 'legal',\n",
    " 'military',\n",
    " 'movement',\n",
    " 'pain',\n",
    " 'passive',\n",
    " 'pleasure',\n",
    " 'politics',\n",
    " 'power',\n",
    " 'religion',\n",
    " 'space',\n",
    " 'time',\n",
    " 'transportation',\n",
    " 'vice',\n",
    " 'weather',\n",
    " 'workandemployment' ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having obtained this new filtered data frame, we are ready to calculate the principal components. \n",
    "\n",
    "`sklearn` is a library which contains many methods in the field of machine learning. It should be installed already when you have downloaded the Anaconda distribution of Python. This library also contains a number of methods for working with PCA. \n",
    "\n",
    "First of all, a new pca object needs to be created. While creating this object, you need to specify the number of components that you want to work with. This pca object has a method named `fit_transform()`. When you supply a data frame as its parameter, the output will be a set of principal components. \n",
    "\n",
    "The first two principal component can then be visualised using a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(combined)\n",
    "pcDf = pd.DataFrame(data = principalComponents , columns = ['pc1', 'pc2'])\n",
    "\n",
    "\n",
    "## add titles and categories from metadata.csv\n",
    "\n",
    "df = pd.read_csv( 'metadata.csv' )\n",
    "pcDf['title'] = df['title']\n",
    "pcDf['class'] = df['class']\n",
    "\n",
    "\n",
    "## plot principal component in a scatter plot\n",
    "\n",
    "colours = [ '#09349E' , '#D1AC32' , '#C70C2B' , '#6AD964' , '#A640E6' ]\n",
    "\n",
    "classColours = dict()\n",
    "\n",
    "unique_categories = list( set( pcDf['class'] ) )\n",
    "if len( unique_categories ) <= len(colours):\n",
    "    for u in range( len( unique_categories ) ):\n",
    "        classColours[ unique_categories[u] ] = colours[u]\n",
    "else:\n",
    "    print(\"You have more than five categories. You need to add colours to the list!\")\n",
    "    \n",
    "colours = []\n",
    "for category in df['class']:\n",
    "    colours.append( classColours[category] )\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = plt.axes()\n",
    "ax.set_xlabel('PC21', fontsize = 14)\n",
    "ax.set_ylabel('PC2', fontsize = 14)\n",
    "ax.set_title('Principal Component Analysis')\n",
    "ax.scatter( pcDf['pc1'], pcDf[ 'pc2'] , s = 70 , color = colours  )\n",
    "\n",
    "for index, row in pcDf.iterrows():\n",
    "    plt.text( row['pc1'] - 0.005 , row['pc2'] + 0.0009 , row['title'] ) \n",
    "\n",
    "patchList = []\n",
    "for key in classColours:\n",
    "    data_key = mpatches.Patch(color=classColours[key], label=key)\n",
    "    patchList.append(data_key)\n",
    "    \n",
    "plt.legend(handles=patchList , shadow=True, fontsize='large' , frameon = True )\n",
    "    \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text which are positioned in close vicinity also have similar values for the variables that have been considered. \n",
    "\n",
    "It is also possible to compare texts on the basis of the words they use. Such comparisons on the basis of vocabulary can be performed effectively on the basis of a term-document matrix. This is document which captures the frequencies of certain words for a collection of texts. It is typically a CSV in which the rows represent all the texts, and in which the columns are words. The values in this CSV file represent the frequencies of these words. When we have such a term-document matrix, we can summarise all these different values by making use of principal component analysis. \n",
    "\n",
    "The code below can be used to create a term-document matrix for all the texts in your corpus. The code firstly identifies the 150 most frequent words in the full corpus. Next, it calculates all the frequencies of these words within individual texts. \n",
    "\n",
    "The results are saved in a file named 'tdm.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os \n",
    "import re\n",
    "\n",
    "dir = 'Corpus'\n",
    "numberOfWords = 150\n",
    "freq = dict()\n",
    "mfw = []\n",
    "\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "print( 'Finding the {} most frequent words in the corpus ...'.format( numberOfWords ) )\n",
    "\n",
    "for file in os.listdir(dir):\n",
    "    if re.search( r'\\.txt$' , file ):\n",
    "        fullText = open( os.path.join( dir , file ) , encoding = 'utf-8' , errors = 'ignore' ).read()\n",
    "        words = word_tokenize(fullText)\n",
    "        for w in words:\n",
    "            if w not in stopWords and re.search( '\\w' , w ):\n",
    "                freq[w.lower()] = freq.get( w.lower() , 0 ) + 1\n",
    "                \n",
    "def sortedByValue( dict ):      \n",
    "    return sorted( dict , key=lambda x: dict[x]) \n",
    "\n",
    "count = 0 \n",
    "for w in reversed(sortedByValue(freq)):\n",
    "    mfw.append(w)\n",
    "    count += 1\n",
    "    if count == numberOfWords:\n",
    "        break\n",
    "        \n",
    "  \n",
    "print( 'Writing the tdm csv file ... ')\n",
    "\n",
    "out = open( 'tdm.csv' , 'w' , encoding = 'utf-8' )\n",
    "out.write( 'title' )\n",
    "\n",
    "for i in range( 0 , numberOfWords ):\n",
    "    out.write( ',' + mfw[i] )\n",
    "out.write('\\n')\n",
    "    \n",
    "        \n",
    "for file in os.listdir(dir):\n",
    "    if re.search( r'\\.txt$' , file ):\n",
    "        print( 'Calculating the word frequencies for {} ...'.format( file ) )\n",
    "        freq = dict() \n",
    "        fullText = open( os.path.join( dir , file ) , encoding = 'utf-8' , errors = 'ignore' ).read()\n",
    "        words = word_tokenize(fullText)\n",
    "        for w in words:\n",
    "            freq[w.lower()] = freq.get( w.lower() , 0 ) + 1\n",
    "        out.write( file + ',')\n",
    "        for i in range( 0 , numberOfWords ):\n",
    "            out.write( str( round( freq.get( mfw[i] , 0 ) / len(words) , 5 )   ) )\n",
    "            if i < numberOfWords-1:\n",
    "                out.write(',')\n",
    "            else:\n",
    "                out.write('\\n')\n",
    "                \n",
    "                \n",
    "out.close()          \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below calculates the principal components and visualises the first two components on a scatter plot. The scatter plot gives an impression of the overall distribution of the values for the variables that have been analysed. Texts which are near to each other in the graph roughly use the same words in the same frequencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv( 'tdm.csv' )\n",
    "del df['title']\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(df)\n",
    "pcDf = pd.DataFrame(data = principalComponents , columns = ['pc1', 'pc2'])\n",
    "\n",
    "\n",
    "df = pd.read_csv( 'metadata.csv' )\n",
    "pcDf['title'] = df['title']\n",
    "pcDf['class'] = df['class']\n",
    "\n",
    "\n",
    "## plot principal component in a scatter plot\n",
    "\n",
    "colours = [ '#09349E' , '#D1AC32' , '#C70C2B' , '#6AD964' , '#A640E6' ]\n",
    "\n",
    "classColours = dict()\n",
    "\n",
    "unique_categories = list( set( pcDf['class'] ) )\n",
    "if len( unique_categories ) <= len(colours):\n",
    "    for u in range( len( unique_categories ) ):\n",
    "        classColours[ unique_categories[u] ] = colours[u]\n",
    "else:\n",
    "    print(\"You have more than five categories. You need to add colours to the list!\")\n",
    "    \n",
    "colours = []\n",
    "for category in df['class']:\n",
    "    colours.append( classColours[category] )\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = plt.axes()\n",
    "ax.set_xlabel('PC21', fontsize = 14)\n",
    "ax.set_ylabel('PC2', fontsize = 14)\n",
    "ax.set_title('Principal Component Analysis')\n",
    "ax.scatter( pcDf['pc1'], pcDf[ 'pc2'] , s = 70 , color = colours  )\n",
    "\n",
    "for index, row in pcDf.iterrows():\n",
    "    plt.text( row['pc1'] - 0.005 , row['pc2'] + 0.0009 , row['title'] ) \n",
    "\n",
    "patchList = []\n",
    "for key in classColours:\n",
    "    data_key = mpatches.Patch(color=classColours[key], label=key)\n",
    "    patchList.append(data_key)\n",
    "    \n",
    "plt.legend(handles=patchList , shadow=True, fontsize='large' , frameon = True )\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Cluster Analysis\n",
    "\n",
    "Diagrams which visualise the results of PCA analyses can expose similarities between texts. When two texts are shown on the scatter plot in close proximity, this implies that the data values that have been calculated for these texts are also similar.\n",
    "\n",
    "Similarity can be established using a range of statistical methods, however. Euclidean distance and Cosine similarity are two statistical methods that can be used to measure similarities. \n",
    "\n",
    "The sklearn library, which was also mentioned above, contains the methods cosine_similarity() and euclidean_distances() which can calculate these similarity metrics. Both methods can be used on data frames, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "df = pd.read_csv( 'tdm.csv' )\n",
    "\n",
    "titles = []\n",
    "\n",
    "for t in df['title'].tolist():\n",
    "    t = re.sub( r'\\.txt$' , '' , t)\n",
    "    titles.append(t)\n",
    "\n",
    "df = df.drop(['title'], axis=1)\n",
    "\n",
    "\n",
    "similarity = euclidean_distances(df)\n",
    "\n",
    "## Or alternatively:\n",
    "similarity = cosine_similarity(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The euclidean_distances() method results in a matrix whose values express the\n",
    "degree of similarity between all texts in the data set. On the basis of such similarity\n",
    "measures, the documents can be grouped together. \n",
    "\n",
    "The results of such similarity measures are often visualised using diagrams which\n",
    "are referred to as dendrograms. Dendrograms divide corpora into clusters, based on\n",
    "an analysis of the overall differences and similarities between texts. In such\n",
    "dendrograms, the texts which are most similar form a\n",
    "single branch, and texts which display fewer similarities do not form a union until a\n",
    "much later stage. The method provides a highly intuitive method of\n",
    "clarifying the differences and the similarities between texts. Dendrograms can be\n",
    "created using the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "linkages = linkage(similarity,'ward')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "dendrogram( linkages , labels = titles , orientation=\"right\", leaf_font_size=8, leaf_rotation=20)\n",
    "plt.tick_params(axis='x', which='both', bottom=False,\n",
    "top=False, labelbottom=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
