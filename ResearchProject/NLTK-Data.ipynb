{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK - Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If you have never used NLTK before, you need to run the code below to install the relevant components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below can be used to add POS tags to all the texts in your corpus. It will create new fileswith the extension 'pos'. Running this code may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "dir = 'Corpus'\n",
    "\n",
    "for file in os.listdir(dir):\n",
    "    if re.search( 'txt$' , file ):\n",
    "        print('Creating POS Tags for ' + file + ' ... ' )\n",
    "        fullText = open( join( dir , file ) , encoding= 'utf-8' ).read()\n",
    "        outFile = re.sub( 'txt$' , 'pos' , file )\n",
    "        out = open( join( dir , outFile ) , 'w' ,  encoding= 'utf-8' )\n",
    "        \n",
    "        sentences = sent_tokenize(fullText)\n",
    "        for s in sentences:\n",
    "            words = word_tokenize(s)\n",
    "            pos = nltk.pos_tag(words)\n",
    "\n",
    "            for p in pos:\n",
    "                out.write( p[0] + '/' + p[1] + ' ' )\n",
    "            out.write('\\n' )\n",
    "        out.close()\n",
    "\n",
    "        \n",
    "print('Done')        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below defines a number of methods. Simply the code below to make sure that you can use these methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "pos = dict()\n",
    "\n",
    "\n",
    "def getTitle( fileName ):\n",
    "    fileName = os.path.basename(fileName)\n",
    "    title = re.sub( r'[.]txt$' , '' , fileName )\n",
    "    return title\n",
    "\n",
    "def countTokens(text):\n",
    "    words = word_tokenize(text)\n",
    "    return len(words)\n",
    "    \n",
    "def averageSentenceLength(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return countTokens(text) / len(sentences)\n",
    "\n",
    "def typeTokenRatio(text , cap ):\n",
    "    words = word_tokenize(text)\n",
    "    words = words[ 0 : cap ]\n",
    "    unique = dict()\n",
    "    for w in words:\n",
    "        unique[w] = unique.get( w, 0 ) + 1\n",
    "    return len(unique) / len(words)\n",
    "\n",
    "def countPosTags( text , tagList ):\n",
    "    count = 0 \n",
    "    fileName = re.sub( 'txt$' , 'pos' , text )\n",
    "    posFile = open( fileName ,  encoding= 'utf-8' ) \n",
    "    for p in posFile:\n",
    "        words = word_tokenize(p)\n",
    "        for w in words:\n",
    "            if re.search( '/' , w ):\n",
    "                token = w[ 0 : w.index('/') ]\n",
    "                tag = w[ w.index('/')+1 : len(w)  ]\n",
    "                if tag in tagList:\n",
    "                    count += 1\n",
    "    return count\n",
    "\n",
    "def showPosTags( text , tagList ):\n",
    "    tokens = []\n",
    "    fileName = re.sub( 'txt$' , 'pos' , text )\n",
    "    posFile = open( fileName ,  encoding= 'utf-8' ) \n",
    "    for p in posFile:\n",
    "        words = word_tokenize(p)\n",
    "        for w in words:\n",
    "            if re.search( '/' , w ):\n",
    "                token = w[ 0 : w.index('/') ]\n",
    "                tag = w[ w.index('/')+1 : len(w)  ]\n",
    "                if tag in tagList:\n",
    "                    tokens.append( token )\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def countPosTags2( text , tagList ):\n",
    "    count = 0 \n",
    "    fileName = re.sub( 'txt$' , 'pos' , text )\n",
    "    posFullText = open( fileName ,  encoding= 'utf-8' ).read()\n",
    "    for t in tagList:\n",
    "        hits = re.findall( r'[/]{}\\b'.format( t ) , posFullText )\n",
    "        count += len(hits)\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "def numberOfSyllables(text):\n",
    "    nrSyllables = 0 \n",
    "    words = word_tokenize(text)\n",
    "    for w in words:\n",
    "        syll = countSyllables(w)\n",
    "        if syll > 1:\n",
    "            nrSyllables += countSyllables(w) \n",
    "        elif( re.search( r'\\w' , w ) ):\n",
    "            nrSyllables += 1\n",
    "    return nrSyllables\n",
    "\n",
    "def countSyllables( word ):\n",
    "    pattern = \"e?[aiou]+e*|e(?!d$|ly).|[td]ed|le$|ble$|a$\"\n",
    "    syllables = re.findall( pattern , word )\n",
    "    return len(syllables)\n",
    "\n",
    "def fleschKincaid( text ):\n",
    "\n",
    "    totalWords = countTokens(text)\n",
    "    totalSentences = len( sent_tokenize(text) )\n",
    "    totalSyllables = numberOfSyllables( text )\n",
    "\n",
    "    fk = 0.39 * (  totalWords / totalSentences )\n",
    "    fk = fk + 11.8 * ( totalSyllables / totalWords )\n",
    "    fk = fk - 15.59\n",
    "    return fk\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the following code to make a first data set in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import re\n",
    "\n",
    "dir = 'Corpus'\n",
    "texts = []\n",
    "pos = []\n",
    "\n",
    "out = open( 'nltk.csv' , 'w' ,  encoding= 'utf-8' )\n",
    "\n",
    "out.write( 'title,tokens,avgSentLength,ttr,adjectives,adverbs,nrSyllables,fk\\n' )\n",
    "\n",
    "\n",
    "for file in os.listdir(dir):\n",
    "    if re.search( 'txt$' , file ):\n",
    "        texts.append( join( dir , file ) )\n",
    "\n",
    "for text in texts:\n",
    "    print( 'Collecting data for ' + text + ' ... ')\n",
    "    fullText = open( text , encoding = 'utf-8' ).read()\n",
    "    out.write( getTitle( text ) + ',' )\n",
    "    tokens = countTokens(fullText)\n",
    "    out.write( '{},'.format( tokens ) )\n",
    "    out.write( '{},'.format( averageSentenceLength(fullText)) )\n",
    "    out.write( '{},'.format( typeTokenRatio(fullText , 1000 )) )\n",
    "    adjectives = [ 'JJ' , 'JJR' , 'JJS' ]\n",
    "    nrAdjectives = countPosTags( text , adjectives )\n",
    "    adverbs = [ 'RB' , 'RBR' , 'RBS' ]\n",
    "    nrAdverbs = countPosTags( text , adverbs )\n",
    "    out.write( '{},'.format( nrAdjectives / tokens  ) )\n",
    "    out.write( '{},'.format( nrAdverbs / tokens  ) )\n",
    "    out.write( '{},'.format( numberOfSyllables(fullText) ) )  \n",
    "    out.write( '{}'.format( fleschKincaid(fullText) ) )      \n",
    "    \n",
    "    out.write( '\\n' )\n",
    "    \n",
    "out.close()    \n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the CSV that was just created, a number of basic visualisations can be produced. The cell below creates a bar chart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv( 'nltk.csv' , index_col=False , decimal = '.' )\n",
    "\n",
    "\n",
    "fig = plt.figure( figsize=( 7 ,6 ) )\n",
    "ax = plt.axes()\n",
    "\n",
    "\n",
    "bar_width = 0.45\n",
    "opacity = 0.8\n",
    "\n",
    "ax.bar( df['title'] , df['avgSentLength'] , width = bar_width, alpha = opacity , color = '#781926')\n",
    "\n",
    "plt.xticks(rotation= 90)\n",
    "\n",
    "ax.set_xlabel('Label for x-axis' , fontsize= 12)\n",
    "ax.set_ylabel('Label for y-axis' , fontsize = 12 )\n",
    "ax.set_title( 'Title for bar chart' , fontsize=20 )\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below creates a scatter plot, which can be used to compare the values of two variables. The two variables to be plotted are defined by the two variables named 'x' and 'y'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv( 'nltk.csv' , index_col=False , decimal = '.' )\n",
    "\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "x = 'avgSentLength'\n",
    "y = 'fk'\n",
    "\n",
    "\n",
    "fig = plt.figure( figsize = ( 6,6 ))\n",
    "ax = plt.axes()\n",
    "\n",
    "\n",
    "ax.scatter(  df[x]  , df[y] , alpha=0.8,  s=80)\n",
    "\n",
    "\n",
    "#for index, column in df.iterrows():\n",
    "#    ax.annotate( df[x]  , df[y] , df['title'] )\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    plt.text( row[x], row[y] , row['title'] , fontsize=12)\n",
    "\n",
    "\n",
    "\n",
    "ax.set_xlabel('Label for x-axis' , fontsize = 16 )\n",
    "ax.set_ylabel('Label for y-axis' , fontsize = 16 )\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
