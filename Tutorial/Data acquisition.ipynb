{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< [Regular expressions](https://tdm.universiteitleiden.nl/Python/Regular%20expressions.html) | [Table of contents](https://tdm.universiteitleiden.nl/Python/) | [NLTK](https://tdm.universiteitleiden.nl/Python/NLTK.html) >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data acquisition\n",
    "\n",
    "\n",
    "Data science projects typically start with the acquisition of data. In many cases, such data sets consist of secondary data made available by commercial or non-commercial organisations. \n",
    "\n",
    "\n",
    "## Direct downloads\n",
    "\n",
    "The methode 'urlopen()', from the module urllib.request can be used to download files from the web. The code in the cell below downloads a given text from project Gutenberg, using the url of this file. If succesful, the 'urlopen()' method creates a new HTTPResponse object, which represents the file that was downloaded. Its function can be compared to that of a file handler. The contents of the HTTPResponse object can be accessed using the 'read()' method. By default, this method renders the contents of the downloaded file as a bytes object. The contents can be processed more effectively if it decoded into a text with UTF-8 encoding, using the decode() method. \n",
    "\n",
    "When you run the code that is given below, the contents of the webpage that is specified at the beginning becomes available as a string, assigned to the variable 'contents'.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"http://www.gutenberg.org/files/98/98-0.txt\"\n",
    "\n",
    "request = urllib.request.urlopen(url)\n",
    "\n",
    "print( type( request ) )\n",
    "\n",
    "bytes = request.read()\n",
    "contents = bytes.decode(\"utf-8\")\n",
    "request.close()\n",
    "\n",
    "print(contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Acquiring data via APIs\n",
    "\n",
    "\n",
    "Organisations which aim to make their data available for reuse often do this through an Application Programming Interface (API). An API, simply put, is a software application which can process online requests for information. These are typically request for specific data sets. The communication between the sender and the recipient of such requests needs to take place according to a protocol; the requests need to formulated according to certain rules. As part of the protocol, the API can respond to incoming queries by returning the requested data in the format that was specified. An API, in short, is a definition of an application which enables organisations to share some of the data that they have with other parties, so that these other external parties can make use of these data in new applications. For many APIs, you need to create an account before you can send reuqests. This is the case, for instance, for the Twitter API. There are also APIs which are fully open, however. \n",
    "\n",
    "The Wikidata API, for instance, is fully open. You can send request to the API without having to provide an access key. This API can be used to find information about Wikipedia pages. More details about the precise format of the requests that you can send to Wikipedia can be found at the [documentation pages](https://www.mediawiki.org/wiki/API). They explain, among other things, that you can use the 'opensearch' function, in which the 'search' parameter needs to be assigned a specific search term. Once the request of this kind has been received, the API will return all the Wikipedia lemmas whose titles contain the provided search term. The API call can be sent to the Wikipedia API using roughly the same methods as those used for direct downloads. \n",
    "\n",
    "In the code below, the JSON data that is sent by Wikidata is processed further using the 'loads()' method from the 'json' library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "apiURL = 'https://en.wikipedia.org/w/api.php?action=opensearch&format=json&search='\n",
    "searchTerm = \"amsterdam\"\n",
    "\n",
    "apiCall = apiURL + searchTerm\n",
    "\n",
    "wikiHeader = {'User-Agent':'p.a.f.verhaar@hum.leidenuniv.nl'}\n",
    "\n",
    "wikiRequest = urllib.request.Request(apiCall, headers=wikiHeader)\n",
    "\n",
    "request = urllib.request.urlopen(wikiRequest)\n",
    "responseData = request.read().decode(\"utf-8\")\n",
    "request.close()\n",
    "\n",
    "print(responseData)\n",
    "\n",
    "\n",
    "wikiResults = json.loads(responseData)\n",
    "\n",
    "\n",
    "for i in range( 0 , len(wikiResults[1]) ):\n",
    "    print( 'Title: ' + wikiResults[1][i] )\n",
    "    print( 'Tagline: ' + wikiResults[2][i] )\n",
    "    print( 'Url: ' + wikiResults[3][i] + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The API developed by [Open Street Map](https://www.openstreetmap.nl/) is also open. The 'search' function of this API needs to used in combination with a textual decsription of an address. The API can return, among other things, the geographic coordinates of the address that is mentioned. Such data can evidently be very useful in GIS applications. \n",
    "\n",
    "In this case, the data are returned in the XML format. The data in this format are processed using the 'xml.etree.ElementTree' module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import string\n",
    "from os.path import isfile, join , isdir\n",
    "import os\n",
    "\n",
    "addresses = ['Grote Looiersstraat 17 Maastricht' , 'Witte Singel 27 Leiden']\n",
    "\n",
    "\n",
    "for a in addresses:\n",
    "    print(a)\n",
    "    url = 'https://nominatim.openstreetmap.org/search?q='+ a + '&format=xml'\n",
    "    url = re.sub( '\\s+' , '%20' , url )\n",
    "\n",
    "    fp = urllib.request.urlopen( url )\n",
    "    mybytes = fp.read()\n",
    "    result = mybytes.decode(\"utf8\")\n",
    "    fp.close()\n",
    "    root = ET.fromstring(result)\n",
    "    el = root.findall('place')\n",
    "    \n",
    "    count = 0\n",
    "    if el is not None:\n",
    "        for place in el:\n",
    "            count += 1\n",
    "            lat = place.attrib['lat']\n",
    "            lon = place.attrib['lon']\n",
    "            if count == 1:\n",
    "                print( '{},{}\\n'.format( lat , lon ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping\n",
    "\n",
    "\n",
    "\n",
    "When a website does not offer access to its structured data via a well-defined API, it may be an option to acquire the data that can be viewed on a site by making use of web scraping. It is a process in which a computer program tries to process the contents of given webpage, and to extract the data values that are needed. The function of a Web scraping application can be compared to that of a web crawler or a bot. The aim of such an application is generally to copy information on a web page and to paste it into a local database. \n",
    "\n",
    "One of the libraries that you can use in Python for scraping online resources is [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/). \n",
    "\n",
    "To scrape webpages, you firstly need to download them, using the code that was explained above. Once you have obtained the contents of a webpage, in the form of an HTML document, you can process its contents effectively by transforming it into a BeautifulSoup object. This object has a ‘find_all()’ function, which needs to be associated with a specific HTML tag. This function returns all occurrences of the tag that is mentioned. To extract all the links on a webpage, for instance, you need to extract all the ‘a’ tags which have the ‘href’ attribute. \n",
    "\n",
    "The code scrapes data from a page on Project Gutenberg. It extract all the titles and the urls of the books which are listed on [Project Gutenberg’s Philosophy Bookshelf](https://www.gutenberg.org/wiki/Philosophy_(Bookshelf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "\n",
    "soup = \"\"\n",
    "\n",
    "## Project Gutenberg Bookshelves can be found at \n",
    "## https://www.gutenberg.org/wiki/Category:Bookshelf\n",
    "\n",
    "url = 'https://www.gutenberg.org/wiki/Philosophy_(Bookshelf)'\n",
    "\n",
    "\n",
    "\n",
    "request = urllib.request.urlopen(url)\n",
    "bytes = request.read()\n",
    "contents = bytes.decode(\"utf-8\")\n",
    "request.close()\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(contents,\"lxml\")\n",
    "\n",
    "\n",
    "links = soup.find_all(\"a\")\n",
    "\n",
    "for l in links:\n",
    "    linktext = l.string\n",
    "    url = l.get(\"href\")\n",
    "\n",
    "    if re.search('gutenberg' , str(url) , re.IGNORECASE):\n",
    "        print(f\"{linktext}: {url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< [Regular expressions](https://tdm.universiteitleiden.nl/Python/Regular%20expressions.html) | [Table of contents](https://tdm.universiteitleiden.nl/Python/) | [NLTK](https://tdm.universiteitleiden.nl/Python/NLTK.html) >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
